# æ–‡ä»¶ï¼šdataset_generator.py
# è¯´æ˜ï¼šåŸºäºçœŸå®é€»è¾‘è§„åˆ™çš„LSATé£æ ¼æ•°æ®é›†ç”Ÿæˆå™¨ï¼ˆé‡æ„ç‰ˆï¼‰

import json
import logging
import random
from typing import List, Dict, Any, Optional
from pathlib import Path

from dag.dag_builder import build_reasoning_dag
from dag.validator import validate_logical_steps
from distractor.generator import DistractorGenerator
from api_key.llm_dispatcher import LLMDispatcher
from utils.consistency_validator import ConsistencyValidator
from utils.enhanced_prompt_builder import EnhancedPromptBuilder
from utils.safe_variable_extractor import SafeVariableExtractor


class DatasetGenerator:
    """
    æ”¹è¿›çš„æ•°æ®é›†ç”Ÿæˆå™¨ï¼šåŸºäºçœŸå®é€»è¾‘è§„åˆ™ç”Ÿæˆé«˜è´¨é‡LSATé£æ ¼é¢˜ç›®
    """

    def __init__(self, llm_dispatcher: LLMDispatcher, prompt_template_path: str):
        """
        åˆå§‹åŒ–æ•°æ®é›†ç”Ÿæˆå™¨

        :param llm_dispatcher: LLMè°ƒåº¦å™¨å®ä¾‹
        :param prompt_template_path: promptæ¨¡æ¿æ–‡ä»¶è·¯å¾„
        """
        self.llm = llm_dispatcher
        self.logger = logging.getLogger("improved_dataset_generator")
        self.extractor = SafeVariableExtractor()
        self.validator = ConsistencyValidator(strictness_level="medium")
        self.prompt_builder = EnhancedPromptBuilder(prompt_template_path)

        # è´¨é‡æ§åˆ¶å‚æ•°
        self.max_retry_attempts = 5
        self.min_valid_steps = 2
        self.max_valid_steps = 6

    def _extract_variables_from_dag(self, root_node) -> List[str]:
        """ä»DAGä¸­æå–æ‰€æœ‰å˜é‡å"""
        return self.extractor.extract_from_dag(root_node)

    def _generate_semantic_bindings(self, variables: List[str]) -> Dict[str, str]:
        """ä¸ºå˜é‡ç”Ÿæˆè¯­ä¹‰ç»‘å®šï¼Œç¡®ä¿è¯­ä¹‰åŸŸä¸€è‡´æ€§"""

        # æ‰©å±•çš„è¯­ä¹‰åŸŸï¼Œæ¯ä¸ªåŸŸéƒ½åŒ…å«å®Œæ•´çš„é€»è¾‘åœºæ™¯
        semantic_domains = {
            "academic_evaluation": {
                "domain_name": "å­¦æœ¯è¯„ä¼°",
                "variables": [
                    "passed_midterm_exam", "submitted_research_paper", "attended_seminars",
                    "completed_assignments", "received_recommendation", "qualified_for_thesis",
                    "defended_thesis", "earned_degree", "published_paper", "won_scholarship"
                ],
                "context_template": "å­¦æœ¯è¯„ä¼°ç³»ç»Ÿä¸­çš„å­¦ç”Ÿè¡¨ç°è¯„ä»·"
            },
            "business_workflow": {
                "domain_name": "å•†ä¸šæµç¨‹",
                "variables": [
                    "project_approved", "budget_allocated", "team_assembled",
                    "milestone_completed", "client_satisfied", "contract_signed",
                    "payment_received", "quality_assured", "deadline_met", "profit_achieved"
                ],
                "context_template": "ä¼ä¸šé¡¹ç›®ç®¡ç†å’Œä¸šåŠ¡æµç¨‹"
            },
            "legal_procedure": {
                "domain_name": "æ³•å¾‹ç¨‹åº",
                "variables": [
                    "evidence_submitted", "witness_testified", "case_filed",
                    "hearing_scheduled", "motion_granted", "settlement_reached",
                    "judgment_rendered", "appeal_filed", "precedent_cited", "verdict_delivered"
                ],
                "context_template": "æ³•å¾‹è¯‰è®¼ç¨‹åºå’Œæ¡ˆä»¶å¤„ç†"
            },
            "medical_diagnosis": {
                "domain_name": "åŒ»ç–—è¯Šæ–­",
                "variables": [
                    "symptoms_observed", "tests_conducted", "results_analyzed",
                    "diagnosis_confirmed", "treatment_prescribed", "patient_responded",
                    "recovery_noted", "followup_scheduled", "clearance_given", "discharge_approved"
                ],
                "context_template": "åŒ»ç–—è¯Šæ–­å’Œæ²»ç–—æµç¨‹"
            },
            "certification_process": {
                "domain_name": "è®¤è¯æµç¨‹",
                "variables": [
                    "training_completed", "exam_passed", "experience_verified",
                    "application_submitted", "review_conducted", "interview_passed",
                    "certification_granted", "license_issued", "renewal_required", "compliance_met"
                ],
                "context_template": "ä¸“ä¸šè®¤è¯å’Œèµ„è´¨è·å–æµç¨‹"
            }
        }

        # éšæœºé€‰æ‹©ä¸€ä¸ªè¯­ä¹‰åŸŸ
        domain_key = random.choice(list(semantic_domains.keys()))
        domain = semantic_domains[domain_key]

        self.logger.info(f"é€‰æ‹©è¯­ä¹‰åŸŸ: {domain['domain_name']}")

        bindings = {}
        domain_vars = domain["variables"]

        # ä¸ºæ¯ä¸ªå˜é‡åˆ†é…è¯­ä¹‰ç»‘å®š
        for i, var in enumerate(variables[:len(domain_vars)]):
            bindings[var] = domain_vars[i]

        # å¦‚æœå˜é‡å¤ªå¤šï¼Œä½¿ç”¨é€šç”¨å‘½å
        if len(variables) > len(domain_vars):
            for i, var in enumerate(variables[len(domain_vars):], 1):
                bindings[var] = f"{domain['domain_name']}_additional_condition_{i}"

        return bindings

    def _format_z3_expressions_improved(self, logical_steps: List[Dict], var_bindings: Dict[str, str]) -> List[str]:
        """æ”¹è¿›çš„Z3è¡¨è¾¾å¼æ ¼å¼åŒ–"""
        z3_exprs = []

        try:
            # 1. å˜é‡å£°æ˜ï¼ˆä½¿ç”¨è¯­ä¹‰åŒ–å˜é‡åï¼‰
            for var, semantic in var_bindings.items():
                z3_exprs.append(f"{semantic} = Bool('{semantic}')")

            # 2. è§„åˆ™è¡¨è¾¾å¼ï¼ˆæ”¹è¿›é”™è¯¯å¤„ç†ï¼‰
            for step in logical_steps:
                try:
                    premises_expr = step.get('premises_expr', [])
                    conclusion_expr = step.get('conclusion_expr')
                    rule_name = step.get('rule', 'Unknown')

                    if not premises_expr or conclusion_expr is None:
                        continue

                    # å®‰å…¨åœ°è½¬æ¢è¡¨è¾¾å¼ä¸ºå­—ç¬¦ä¸²
                    premise_strs = []
                    for premise in premises_expr:
                        try:
                            # å°†Z3å˜é‡åæ›¿æ¢ä¸ºè¯­ä¹‰å
                            premise_str = str(premise)
                            for var, semantic in var_bindings.items():
                                premise_str = premise_str.replace(var, semantic)
                            premise_strs.append(premise_str)
                        except Exception as e:
                            self.logger.debug(f"è½¬æ¢å‰æè¡¨è¾¾å¼å¤±è´¥: {e}")
                            continue

                    if not premise_strs:
                        continue

                    # è½¬æ¢ç»“è®ºè¡¨è¾¾å¼
                    try:
                        conclusion_str = str(conclusion_expr)
                        for var, semantic in var_bindings.items():
                            conclusion_str = conclusion_str.replace(var, semantic)
                    except Exception as e:
                        self.logger.debug(f"è½¬æ¢ç»“è®ºè¡¨è¾¾å¼å¤±è´¥: {e}")
                        continue

                    # æ„é€ è•´å«å…³ç³»
                    if len(premise_strs) == 1:
                        z3_exprs.append(f"Implies({premise_strs[0]}, {conclusion_str})")
                    else:
                        premises_conjunction = f"And({', '.join(premise_strs)})"
                        z3_exprs.append(f"Implies({premises_conjunction}, {conclusion_str})")

                except Exception as e:
                    self.logger.debug(f"å¤„ç†é€»è¾‘æ­¥éª¤æ—¶å‡ºé”™: {e}")
                    continue

        except Exception as e:
            self.logger.error(f"æ ¼å¼åŒ–Z3è¡¨è¾¾å¼æ—¶å‡ºé”™: {e}")

        return z3_exprs

    def _create_enhanced_distractors(self, logical_steps: List[Dict], var_bindings: Dict[str, str]) -> List[str]:
        """åˆ›å»ºå¢å¼ºçš„å¹²æ‰°é¡¹"""
        try:
            # åˆ›å»ºå®‰å…¨çš„å¸ƒå°”å˜é‡
            var_names = list(var_bindings.keys())
            safe_vars = self.extractor.create_safe_bool_vars(var_names)

            if not safe_vars:
                self.logger.warning("æ— æ³•åˆ›å»ºæœ‰æ•ˆçš„å¸ƒå°”å˜é‡ï¼Œè·³è¿‡å¹²æ‰°é¡¹ç”Ÿæˆ")
                return self._create_fallback_distractors()

            # ç”Ÿæˆå¹²æ‰°é¡¹
            distractor_gen = DistractorGenerator(
                available_vars=safe_vars,
                enabled_strategies=["illogical_reasoning", "adversarial_structure", "reversed_implication"]
            )

            distractors = distractor_gen.generate_all(logical_steps, num_per_strategy=2)

            # è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°
            distractor_descriptions = []
            for d in distractors[:3]:  # æœ€å¤š3ä¸ªå¹²æ‰°é¡¹
                desc = d.get('description', f"åŸºäº{d.get('strategy', 'unknown')}ç­–ç•¥çš„å¹²æ‰°é¡¹")
                distractor_descriptions.append(desc)

            return distractor_descriptions

        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆå¹²æ‰°é¡¹å¤±è´¥: {e}")
            return self._create_fallback_distractors()

    def _create_fallback_distractors(self) -> List[str]:
        """åˆ›å»ºå¤‡ç”¨å¹²æ‰°é¡¹"""
        return [
            "åŸºäºä¸å®Œæ•´å‰æçš„é”™è¯¯æ¨ç†",
            "é€»è¾‘æ–¹å‘é¢ å€’çš„é”™è¯¯ç»“è®º",
            "æ— å…³æ¡ä»¶çš„å¹²æ‰°æ€§æ¨æ–­"
        ]

    def generate_single_sample(self, max_depth: int = 3) -> Optional[Dict[str, Any]]:
        """ç”Ÿæˆå•ä¸ªæ•°æ®æ ·æœ¬ï¼ˆæ”¯æŒä»»æ„é•¿åº¦é“¾æ¡ï¼‰"""
        for attempt in range(self.max_retry_attempts):
            try:
                # 1. æ„å»ºæ¨ç†DAGï¼ˆè‡ªåŠ¨é€‰æ‹©çŸ­é“¾æ¡æˆ–é•¿é“¾æ¡ï¼‰
                self.logger.info(f"æ„å»ºæ¨ç†DAGï¼Œæ·±åº¦={max_depth}ï¼Œå°è¯•{attempt + 1}")
                root, logical_steps = build_reasoning_dag(max_depth=max_depth, min_depth=max(max_depth // 3, 2))

                if not logical_steps:
                    self.logger.warning("æœªèƒ½ç”Ÿæˆé€»è¾‘æ­¥éª¤ï¼Œé‡è¯•")
                    continue

                # 2. éªŒè¯é€»è¾‘æ­¥éª¤
                valid_steps, failed_steps = validate_logical_steps(logical_steps)

                if len(valid_steps) < self.min_valid_steps:
                    self.logger.warning(f"æœ‰æ•ˆæ­¥éª¤å¤ªå°‘ ({len(valid_steps)} < {self.min_valid_steps})ï¼Œé‡è¯•")
                    continue

                if len(valid_steps) > self.max_valid_steps:
                    valid_steps = valid_steps[:self.max_valid_steps]

                self.logger.info(f"æˆåŠŸéªŒè¯ {len(valid_steps)} ä¸ªé€»è¾‘æ­¥éª¤")

                # 3. æå–å˜é‡å’Œç”Ÿæˆè¯­ä¹‰ç»‘å®š
                variables = self._extract_variables_from_dag(root)
                if not variables:
                    self.logger.warning("æœªèƒ½æå–åˆ°å˜é‡ï¼Œé‡è¯•")
                    continue

                var_bindings = self._generate_semantic_bindings(variables)
                self.logger.info(f"ç”Ÿæˆ {len(var_bindings)} ä¸ªå˜é‡ç»‘å®š")

                # 4. æ„å»ºå¢å¼ºprompt
                enhanced_prompt = self.prompt_builder.build_constrained_prompt(
                    z3_exprs=self._format_z3_expressions_improved(valid_steps, var_bindings),
                    var_bindings=var_bindings,
                    logical_steps=valid_steps,
                    previous_violations=[]
                )

                # 5. è°ƒç”¨LLM
                self.logger.info("è°ƒç”¨LLMç”Ÿæˆé¢˜ç›®...")
                response = self.llm.call(enhanced_prompt)

                if not response:
                    self.logger.error("LLMå“åº”ä¸ºç©ºï¼Œé‡è¯•")
                    continue

                # 6. è§£æå“åº”
                sample = self._parse_llm_response_improved(response, valid_steps, var_bindings)
                if not sample:
                    self.logger.error("å“åº”è§£æå¤±è´¥ï¼Œé‡è¯•")
                    continue

                # 7. è´¨é‡éªŒè¯
                is_valid, violations = self.validator.validate_sample(sample)

                if is_valid:
                    self.logger.info("âœ… æ ·æœ¬é€šè¿‡è´¨é‡éªŒè¯")
                    return self._add_enhanced_metadata(sample, valid_steps, var_bindings)
                else:
                    self.logger.warning(f"âš ï¸ è´¨é‡éªŒè¯å¤±è´¥: {violations}")
                    # åœ¨æœ€åä¸€æ¬¡å°è¯•æ—¶ï¼Œè¿”å›éƒ¨åˆ†åˆæ ¼çš„æ ·æœ¬
                    if attempt == self.max_retry_attempts - 1:
                        sample['validation_warnings'] = violations
                        return self._add_enhanced_metadata(sample, valid_steps, var_bindings)

            except Exception as e:
                self.logger.error(f"ç”Ÿæˆæ ·æœ¬æ—¶å‡ºé”™ (å°è¯• {attempt + 1}): {e}")
                continue

        self.logger.error("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥")
        return None

    def _parse_llm_response_improved(self, response: str, valid_steps: List[Dict], var_bindings: Dict[str, str]) -> \
    Optional[Dict]:
        """æ”¹è¿›çš„LLMå“åº”è§£æ"""
        try:
            # æå–JSONéƒ¨åˆ†
            json_start = response.find('{')
            json_end = response.rfind('}') + 1

            if json_start == -1 or json_end <= json_start:
                self.logger.error("å“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆJSON")
                return None

            json_str = response[json_start:json_end]

            # è§£æJSON
            result = json.loads(json_str)

            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = ["context", "question", "answers", "label", "z3"]
            for field in required_fields:
                if field not in result:
                    self.logger.error(f"å“åº”ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                    return None

            # éªŒè¯answersæ ¼å¼
            if not isinstance(result["answers"], list) or len(result["answers"]) != 4:
                self.logger.error(f"ç­”æ¡ˆé€‰é¡¹æ ¼å¼é”™è¯¯: {result.get('answers')}")
                return None

            # éªŒè¯labelæ ¼å¼
            if result["label"] not in ["A", "B", "C", "D"]:
                self.logger.error(f"æ ‡ç­¾æ ¼å¼é”™è¯¯: {result.get('label')}")
                return None

            return result

        except json.JSONDecodeError as e:
            self.logger.error(f"JSONè§£æå¤±è´¥: {e}")
            # å°è¯•ä¿®å¤å¸¸è§çš„JSONé”™è¯¯
            return self._try_fix_json(json_str)
        except Exception as e:
            self.logger.error(f"è§£æå“åº”æ—¶å‡ºé”™: {e}")
            return None

    def _try_fix_json(self, json_str: str) -> Optional[Dict]:
        """å°è¯•ä¿®å¤å¸¸è§çš„JSONé”™è¯¯"""
        try:
            # ä¿®å¤å¸¸è§é—®é¢˜ï¼šå¤šä½™çš„é€—å·ã€å¼•å·é—®é¢˜ç­‰
            fixed_json = json_str.replace(',]', ']').replace(',}', '}')
            # å°è¯•å†æ¬¡è§£æ
            return json.loads(fixed_json)
        except:
            return None

    def _add_enhanced_metadata(self, sample: Dict, valid_steps: List[Dict], var_bindings: Dict[str, str]) -> Dict:
        """æ·»åŠ å¢å¼ºçš„å…ƒæ•°æ®"""
        sample['metadata'] = {
            'reasoning_depth': len(valid_steps),
            'variables_count': len(var_bindings),
            'rules_used': [step.get('rule', 'Unknown') for step in valid_steps],
            'semantic_domain': self._infer_semantic_domain(var_bindings),
            'logical_complexity': self._calculate_complexity(valid_steps),
            'generation_version': 'improved_v2'
        }

        # æ·»åŠ è´¨é‡åˆ†æ•°
        sample['quality_score'] = self._calculate_quality_score(sample)

        return sample

    def _infer_semantic_domain(self, var_bindings: Dict[str, str]) -> str:
        """æ¨æ–­è¯­ä¹‰åŸŸ"""
        all_bindings = " ".join(var_bindings.values()).lower()

        domain_keywords = {
            "academic": ["exam", "assignment", "grade", "course", "student"],
            "business": ["project", "budget", "client", "contract", "profit"],
            "legal": ["evidence", "witness", "case", "court", "judgment"],
            "medical": ["diagnosis", "treatment", "patient", "symptoms"],
            "certification": ["training", "certification", "license", "compliance"]
        }

        for domain, keywords in domain_keywords.items():
            if any(keyword in all_bindings for keyword in keywords):
                return domain

        return "general"

    def _calculate_complexity(self, valid_steps: List[Dict]) -> str:
        """è®¡ç®—é€»è¾‘å¤æ‚åº¦"""
        step_count = len(valid_steps)

        if step_count <= 2:
            return "simple"
        elif step_count <= 4:
            return "medium"
        else:
            return "complex"

    def _calculate_quality_score(self, sample: Dict) -> float:
        """è®¡ç®—è´¨é‡åˆ†æ•° (0-1)"""
        score = 0.0

        # åŸºç¡€åˆ†æ•°
        score += 0.3

        # è¯­ä¹‰ä¸€è‡´æ€§
        if 'validation_warnings' not in sample:
            score += 0.3

        # é€»è¾‘æ·±åº¦
        depth = sample.get('metadata', {}).get('reasoning_depth', 0)
        score += min(depth / 6, 0.2)  # æœ€å¤š0.2åˆ†

        # å˜é‡ä½¿ç”¨
        var_count = sample.get('metadata', {}).get('variables_count', 0)
        score += min(var_count / 8, 0.2)  # æœ€å¤š0.2åˆ†

        return min(score, 1.0)

    def generate_dataset(self, num_samples: int, output_path: str, max_depth_range: tuple = (5, 12)) -> None:
        """ç”Ÿæˆå®Œæ•´æ•°æ®é›†ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        self.logger.info(f"å¼€å§‹ç”Ÿæˆ {num_samples} ä¸ªæ ·æœ¬çš„æ”¹è¿›æ•°æ®é›†")

        Path(output_path).parent.mkdir(parents=True, exist_ok=True)

        successful_samples = []
        attempts = 0
        max_attempts = num_samples * 4  # å¢åŠ æœ€å¤§å°è¯•æ¬¡æ•°

        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_attempts": 0,
            "successful": 0,
            "failed": 0,
            "quality_scores": [],
            "semantic_domains": {},
            "complexity_levels": {}
        }

        while len(successful_samples) < num_samples and attempts < max_attempts:
            attempts += 1
            stats["total_attempts"] += 1

            depth = random.randint(*max_depth_range)

            self.logger.info(f"ç”Ÿæˆæ ·æœ¬ {len(successful_samples) + 1}/{num_samples} (å°è¯• {attempts})")

            sample = self.generate_single_sample(max_depth=depth)

            if sample:
                successful_samples.append(sample)
                stats["successful"] += 1

                # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
                quality_score = sample.get('quality_score', 0)
                stats["quality_scores"].append(quality_score)

                domain = sample.get('metadata', {}).get('semantic_domain', 'unknown')
                stats["semantic_domains"][domain] = stats["semantic_domains"].get(domain, 0) + 1

                complexity = sample.get('metadata', {}).get('logical_complexity', 'unknown')
                stats["complexity_levels"][complexity] = stats["complexity_levels"].get(complexity, 0) + 1

                self.logger.info(f"âœ… æˆåŠŸç”Ÿæˆæ ·æœ¬ {len(successful_samples)} (è´¨é‡åˆ†æ•°: {quality_score:.2f})")
            else:
                stats["failed"] += 1
                self.logger.warning(f"âŒ æ ·æœ¬ç”Ÿæˆå¤±è´¥ (å°è¯• {attempts})")

        # ä¿å­˜æ•°æ®é›†
        self._save_dataset_with_stats(successful_samples, stats, output_path)

        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        self._print_final_statistics(stats)

    def _save_dataset_with_stats(self, samples: List[Dict], stats: Dict, output_path: str):
        """ä¿å­˜æ•°æ®é›†å¹¶åŒ…å«ç»Ÿè®¡ä¿¡æ¯"""
        # ä¿å­˜æ•°æ®é›†
        with open(output_path, 'w', encoding='utf-8') as f:
            for sample in samples:
                f.write(json.dumps(sample, ensure_ascii=False) + '\n')

        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_path = output_path.replace('.jsonl', '_stats.json')
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        self.logger.info(f"æ•°æ®é›†å·²ä¿å­˜: {output_path}")
        self.logger.info(f"ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_path}")

    def _print_final_statistics(self, stats: Dict):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        success_rate = stats["successful"] / stats["total_attempts"] * 100 if stats["total_attempts"] > 0 else 0
        avg_quality = sum(stats["quality_scores"]) / len(stats["quality_scores"]) if stats["quality_scores"] else 0

        self.logger.info("=" * 50)
        self.logger.info("ğŸ“Š æ•°æ®é›†ç”Ÿæˆç»Ÿè®¡")
        self.logger.info(f"æˆåŠŸç‡: {success_rate:.1f}% ({stats['successful']}/{stats['total_attempts']})")
        self.logger.info(f"å¹³å‡è´¨é‡åˆ†æ•°: {avg_quality:.3f}")
        self.logger.info(f"è¯­ä¹‰åŸŸåˆ†å¸ƒ: {stats['semantic_domains']}")
        self.logger.info(f"å¤æ‚åº¦åˆ†å¸ƒ: {stats['complexity_levels']}")
        self.logger.info("=" * 50)


def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # åˆå§‹åŒ–LLMè°ƒåº¦å™¨
    llm = LLMDispatcher(
        model_name="deepseek-chat",
        api_key_path="api_key/ds-api_key.txt",
        retries=3
    )

    # åˆå§‹åŒ–æ”¹è¿›çš„æ•°æ®é›†ç”Ÿæˆå™¨
    generator = DatasetGenerator(
        llm_dispatcher=llm,
        prompt_template_path="prompt/lsat_prompt.txt"
    )

    # ç”Ÿæˆæ•°æ®é›†
    generator.generate_dataset(
        num_samples=1,
        output_path="output/unified_lsat_dataset.jsonl",
        max_depth_range=(2, 5)  # æ”¯æŒ6-12æ­¥çš„æ¨ç†é“¾
    )


if __name__ == "__main__":
    main()